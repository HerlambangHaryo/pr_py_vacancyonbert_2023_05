{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengimpor library PyTorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Mengimpor library HuggingFace Transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n",
    "\n",
    "# Mengimpor library lainnya\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Mendefinisikan fungsi untuk memisahkan teks dan label menjadi list of tokens\n",
    "def tokenize_text_and_label(text, label):\n",
    "  # Menggunakan tokenizer dari IndoBERT\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "\n",
    "  # Melakukan tokenisasi teks dan label\n",
    "  text_tokens = tokenizer.tokenize(text)\n",
    "  label_tokens = label.split()\n",
    "\n",
    "  # Menyesuaikan panjang label_tokens dengan text_tokens\n",
    "  # Karena tokenizer BERT menggunakan WordPiece, beberapa kata dapat terpecah menjadi beberapa subword\n",
    "  # Misalnya, kata \"Pengalaman\" menjadi [\"peng\", \"##ala\", \"##man\"]\n",
    "  # Untuk kasus ini, kita hanya memberikan label pada subword pertama dan memberikan label \"X\" pada subword lainnya\n",
    "  # Label \"X\" akan diabaikan saat melatih model\n",
    "  new_label_tokens = []\n",
    "  i = 0\n",
    "  for token in text_tokens:\n",
    "    if token.startswith(\"##\"):\n",
    "      new_label_tokens.append(\"X\")\n",
    "    else:\n",
    "      new_label_tokens.append(label_tokens[i])\n",
    "      i += 1\n",
    "\n",
    "  # Mengembalikan text_tokens dan new_label_tokens sebagai output\n",
    "  return text_tokens, new_label_tokens\n",
    "\n",
    "# Mendefinisikan fungsi untuk mengubah list of tokens menjadi list of ids\n",
    "def tokens_to_ids(text_tokens, label_tokens):\n",
    "  # Menggunakan tokenizer dan config dari IndoBERT\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "  config = AutoConfig.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "\n",
    "  # Mengubah text_tokens menjadi input_ids dan attention_mask\n",
    "  # Input_ids adalah representasi numerik dari text_tokens yang dapat dimengerti oleh model BERT\n",
    "  # Attention_mask adalah tensor yang menunjukkan posisi token yang sebenarnya (1) dan yang dipad (0)\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(text_tokens)\n",
    "  attention_mask = [1] * len(input_ids)\n",
    "\n",
    "  # Mengubah label_tokens menjadi label_ids\n",
    "  # Label_ids adalah representasi numerik dari label_tokens yang dapat dimengerti oleh model BERT\n",
    "  # Kita menggunakan dictionary yang telah disediakan oleh config dari IndoBERT\n",
    "  label_map = config.label2id\n",
    "  label_ids = [label_map[label] for label in label_tokens]\n",
    "\n",
    "  # Mengembalikan input_ids, attention_mask, dan label_ids sebagai output\n",
    "  return input_ids, attention_mask, label_ids\n",
    "\n",
    "# Mendefinisikan kelas JobDataset yang merupakan subclass dari kelas Dataset PyTorch\n",
    "class JobDataset(Dataset):\n",
    "  \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Mengambil data pada indeks tertentu\n",
    "        row = self.data.iloc[index]\n",
    "\n",
    "        # Memisahkan teks dan label menjadi list of tokens\n",
    "        text = row[\"text\"]\n",
    "        label = row[\"label\"]\n",
    "        text_tokens, label_tokens = tokenize_text_and_label(text,label)\n",
    "\n",
    "        # Mengubah list of tokens menjadi list of ids\n",
    "        input_ids, attention_mask, label_ids = tokens_to_ids(text_tokens,label_tokens)\n",
    "\n",
    "        # Mengubah list of ids menjadi tensor PyTorch\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "        label_ids = torch.tensor(label_ids)\n",
    "\n",
    "        # Mengembalikan sebuah dictionary yang berisi input_ids, attention_mask, dan label_ids sebagai output\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label_ids\": label_ids\n",
    "        }\n",
    "\n",
    "# Mendefinisikan kelas JobDataLoader yang merupakan subclass dari kelas DataLoader PyTorch\n",
    "class JobDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # Membuat list of indices dari dataset\n",
    "        indices = list(range(len(self.dataset)))\n",
    "\n",
    "        # Melakukan shuffling pada list of indices\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        # Membuat list kosong untuk menyimpan batch data\n",
    "        batch_data = []\n",
    "\n",
    "        # Melakukan iterasi pada setiap indeks dalam list of indices\n",
    "        for i in indices:\n",
    "            # Mengambil data pada indeks tertentu dari dataset dengan menggunakan method __getitem__\n",
    "            data = self.dataset[i]\n",
    "\n",
    "            # Menambahkan data ke list batch data\n",
    "            batch_data.append(data)\n",
    "\n",
    "            # Jika panjang list batch data sudah sama dengan batch size atau sudah mencapai indeks terakhir, maka proses batch data\n",
    "            if len(batch_data) == self.batch_size or i == indices[-1]:\n",
    "                # Mengambil input_ids, attention_mask, dan label_ids dari setiap data dalam batch data\n",
    "                input_ids = [data[\"input_ids\"] for data in batch_data]\n",
    "                attention_mask = [data[\"attention_mask\"] for data in batch_data]\n",
    "                label_ids = [data[\"label_ids\"] for data in batch_data]\n",
    "\n",
    "                # Melakukan padding pada input_ids, attention_mask, dan label_ids agar memiliki panjang yang sama dalam satu batch\n",
    "                # Menggunakan nilai 0 untuk padding input_ids dan attention_mask, dan nilai -100 untuk padding label_ids\n",
    "                # Nilai -100 akan diabaikan oleh loss function saat melatih model\n",
    "                input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "                attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "                label_ids = pad_sequence(label_ids, batch_first=True, padding_value=-100)\n",
    "\n",
    "                # Mengembalikan sebuah dictionary yang berisi input_ids, attention_mask, dan label_ids sebagai output dari iterator\n",
    "                yield {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"label_ids\": label_ids\n",
    "                }\n",
    "\n",
    "                # Mengosongkan list batch data untuk batch selanjutnya\n",
    "                batch_data = []\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
