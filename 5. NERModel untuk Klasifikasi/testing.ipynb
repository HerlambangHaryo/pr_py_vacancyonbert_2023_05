{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn' has no attribute 'model_selection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ASUS\\AryaDataAnalyst\\pr_py_vacancyonbert_2023_05\\5. NERModel untuk Klasifikasi\\testing.ipynb Cell 1\u001b[0m line \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/AryaDataAnalyst/pr_py_vacancyonbert_2023_05/5.%20NERModel%20untuk%20Klasifikasi/testing.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m labels \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mB-PER I-PER I-PER O B-LOC I-LOC O O O O O O O B-DIS O\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/AryaDataAnalyst/pr_py_vacancyonbert_2023_05/5.%20NERModel%20untuk%20Klasifikasi/testing.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Split dataset into train, validation, and test sets\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ASUS/AryaDataAnalyst/pr_py_vacancyonbert_2023_05/5.%20NERModel%20untuk%20Klasifikasi/testing.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m train_sentences, test_sentences, train_labels, test_labels \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39;49mmodel_selection\u001b[39m.\u001b[39mtrain_test_split(sentences, labels, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/AryaDataAnalyst/pr_py_vacancyonbert_2023_05/5.%20NERModel%20untuk%20Klasifikasi/testing.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m train_sentences, val_sentences, train_labels, val_labels \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39mmodel_selection\u001b[39m.\u001b[39mtrain_test_split(train_sentences, train_labels, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ASUS/AryaDataAnalyst/pr_py_vacancyonbert_2023_05/5.%20NERModel%20untuk%20Klasifikasi/testing.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Define function to convert dataset into BERT format\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn' has no attribute 'model_selection'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# Define model and tokenizer\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "model = transformers.BertForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create dataset\n",
    "sentences = [\"Presiden Joko Widodo mengunjungi kota Surabaya pada hari Selasa untuk meninjau vaksinasi massal Covid-19.\"]\n",
    "labels = [\"B-PER I-PER I-PER O B-LOC I-LOC O O O O O O O B-DIS O\"]\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_sentences, test_sentences, train_labels, test_labels = sklearn.model_selection.train_test_split(sentences, labels, test_size=0.2)\n",
    "train_sentences, val_sentences, train_labels, val_labels = sklearn.model_selection.train_test_split(train_sentences, train_labels, test_size=0.2)\n",
    "\n",
    "# Define function to convert dataset into BERT format\n",
    "def convert_to_bert_format(sentences, labels):\n",
    "  input_ids = []\n",
    "  attention_mask = []\n",
    "  label_ids = []\n",
    "  for sentence, label in zip(sentences, labels):\n",
    "    # Tokenize sentence and label\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    label_tokens = label.split()\n",
    "    # Add special tokens\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    label_tokens = [\"O\"] + label_tokens + [\"O\"]\n",
    "    # Convert tokens and labels to ids\n",
    "    input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    label_id = [label_to_id[l] for l in label_tokens] # label_to_id is a dictionary that maps labels to ids\n",
    "    # Create attention mask\n",
    "    att_mask = [1] * len(input_id)\n",
    "    # Pad or truncate to max length\n",
    "    max_length = 128\n",
    "    if len(input_id) > max_length:\n",
    "      input_id = input_id[:max_length]\n",
    "      label_id = label_id[:max_length]\n",
    "      att_mask = att_mask[:max_length]\n",
    "    else:\n",
    "      padding_length = max_length - len(input_id)\n",
    "      input_id = input_id + [0] * padding_length\n",
    "      label_id = label_id + [-100] * padding_length # -100 is the ignore index for the loss function\n",
    "      att_mask = att_mask + [0] * padding_length\n",
    "    # Append to lists\n",
    "    input_ids.append(input_id)\n",
    "    attention_mask.append(att_mask)\n",
    "    label_ids.append(label_id)\n",
    "  # Convert lists to tensors\n",
    "  input_ids = torch.tensor(input_ids)\n",
    "  attention_mask = torch.tensor(attention_mask)\n",
    "  label_ids = torch.tensor(label_ids)\n",
    "  return input_ids, attention_mask, label_ids\n",
    "\n",
    "# Convert train, validation, and test sets into BERT format\n",
    "train_input_ids, train_attention_mask, train_label_ids = convert_to_bert_format(train_sentences, train_labels)\n",
    "val_input_ids, val_attention_mask, val_label_ids = convert_to_bert_format(val_sentences, val_labels)\n",
    "test_input_ids, test_attention_mask, test_label_ids = convert_to_bert_format(test_sentences, test_labels)\n",
    "\n",
    "# Define function to train BERT model\n",
    "def train_bert_model(model, train_input_ids, train_attention_mask, train_label_ids, val_input_ids, val_attention_mask, val_label_ids):\n",
    "  # Define parameters\n",
    "  learning_rate = 2e-5\n",
    "  batch_size = 32\n",
    "  epoch = 3\n",
    "  # Define optimizer, scheduler, and loss function\n",
    "  optimizer = transformers.AdamW(model.parameters(), lr=learning_rate)\n",
    "  scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_input_ids) // batch_size * epoch)\n",
    "  loss_function = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "  # Define device\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model.to(device)\n",
    "  # Define best validation loss\n",
    "  best_val_loss = float(\"inf\")\n",
    "  # Loop over epochs\n",
    "  for e in range(epoch):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    # Initialize training loss\n",
    "    train_loss = 0\n",
    "    # Loop over batches\n",
    "    for i in range(0, len(train_input_ids), batch_size):\n",
    "      # Get batch data\n",
    "      batch_input_ids = train_input_ids[i:i+batch_size].to(device)\n",
    "      batch_attention_mask = train_attention_mask[i:i+batch_size].to(device)\n",
    "      batch_label_ids = train_label_ids[i:i+batch_size].to(device)\n",
    "      # Zero the gradients\n",
    "      optimizer.zero_grad()\n",
    "      # Forward pass\n",
    "      output = model(batch_input_ids, attention_mask=batch_attention_mask, labels=batch_label_ids)\n",
    "      # Get loss\n",
    "      loss = output[0]\n",
    "      # Backward pass\n",
    "      loss.backward()\n",
    "      # Update parameters\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      # Update training loss\n",
    "      train_loss += loss.item()\n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = train_loss / len(train_input_ids)\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    # Initialize validation loss\n",
    "    val_loss = 0\n",
    "    # Loop over batches\n",
    "    for i in range(0, len(val_input_ids), batch_size):\n",
    "      # Get batch data\n",
    "      batch_input_ids = val_input_ids[i:i+batch_size].to(device)\n",
    "      batch_attention_mask = val_attention_mask[i:i+batch_size].to(device)\n",
    "      batch_label_ids = val_label_ids[i:i+batch_size].to(device)\n",
    "      # Forward pass with no gradient calculation\n",
    "      with torch.no_grad():\n",
    "        output = model(batch_input_ids, attention_mask=batch_attention_mask, labels=batch_label_ids)\n",
    "        # Get loss\n",
    "        loss = output[0]\n",
    "        # Update validation loss\n",
    "        val_loss += loss.item()\n",
    "    # Calculate average validation loss\n",
    "    avg_val_loss = val_loss / len(val_input_ids)\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {e+1}: Train loss: {avg_train_loss:.4f}, Val loss: {avg_val_loss:.4f}\")\n",
    "    # Save the best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "      best_val_loss = avg_val_loss\n",
    "      torch.save(model.state_dict(), \"best_model.pt\")\n",
    "  \n",
    "# Train BERT model using train and validation sets\n",
    "train_bert_model(model, train_input_ids, train_attention_mask, train_label_ids, val_input_ids, val_attention_mask, val_label_ids)\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "# Define function to evaluate BERT model using test set\n",
    "def evaluate_bert_model(model, test_input_ids, test_attention_mask, test_label_ids):\n",
    "  # Define device\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model.to(device)\n",
    "  # Set model to evaluation mode\n",
    "  model.eval()\n",
    "  # Initialize predictions and true labels lists\n",
    "  predictions = []\n",
    "  true_labels = []\n",
    "  # Loop over batches\n",
    "  for i in range(0, len(test_input_ids), batch_size):\n",
    "    # Get batch data\n",
    "    batch_input_ids = test_input_ids[i:i+batch_size].to(device)\n",
    "    batch_attention_mask = test_attention_mask[i:i+batch_size].to(device)\n",
    "    batch_label_ids = test_label_ids[i:i+batch_size].to(device)\n",
    "    # Forward pass with no gradient calculation\n",
    "    with torch.no_grad():\n",
    "      output = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "      # Get logits and labels\n",
    "      logits = output[0]\n",
    "      labels = batch_label_ids.cpu().numpy()\n",
    "      # Convert logits to predictions\n",
    "      preds = np.argmax(logits.cpu().numpy(), axis=2)\n",
    "      # Append predictions and true labels to lists\n",
    "      predictions.extend(preds.tolist())\n",
    "      true_labels.extend(labels.tolist())\n",
    "  \n",
    "  return predictions, true_labels\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
