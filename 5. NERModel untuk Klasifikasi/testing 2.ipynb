{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cahya/bert-base-indonesian-1.5G and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Import Library: Pertama, kita mengimpor library yang diperlukan. \n",
    "# Kami menggunakan Hugging Face Transformers, \n",
    "# sebuah library yang memungkinkan kita untuk menggunakan model bahasa berbasis BERT dan \n",
    "# alat pemrosesan bahasa alami.\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "\n",
    "# 2. Load Pre-trained Model dan Tokenizer: \n",
    "# Kami mengunduh model BERT dan tokenizer yang telah dilatih sebelumnya. \n",
    "# Tokenizer digunakan untuk memecah teks menjadi potongan-potongan yang disebut token. \n",
    "# Token ini adalah unit dasar yang dikenali oleh model\n",
    "tokenizer = BertTokenizer.from_pretrained('cahya/bert-base-indonesian-1.5G')\n",
    "model = BertForTokenClassification.from_pretrained('cahya/bert-base-indonesian-1.5G')\n",
    "\n",
    "# Text input\n",
    "\n",
    "# 3. Input Text: Ini adalah teks yang akan kita proses. Teks ini berisi informasi tentang usia maksimal.\n",
    "text = \"Memiliki atau tidak memeliki pengalaman mengajar privat/sekolah untuk Bahasa Jepang Usia maksimal 27 tahun.\"\n",
    "\n",
    "# Tokenize the text\n",
    "# 4. Tokenize Text: Kami memecah teks menjadi token-token. \n",
    "# Token adalah unit bahasa seperti kata atau karakter yang dimengerti oleh model.\n",
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "\n",
    "# Encode the text into IDs\n",
    "# 5. Encode Text: Kami mengonversi daftar token ke dalam representasi numerik yang dapat dipahami oleh model.\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "# Get the token-level predictions\n",
    "# 6. Get Token Predictions: Kami mengirimkan tensor ID token ke model BERT untuk mendapatkan prediksi label token.\n",
    "#  Model BERT digunakan untuk tugas Token Classification, di mana setiap token dalam teks diberi label. Kode ini menghitung label token.\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "# Get the predicted token labels (e.g., 'O', 'B-USIA', 'I-USIA', etc.)\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=2)[0].tolist()\n",
    "\n",
    "# Find the 'USIA' entity and extract its value\n",
    "# 7. Get Token Predictions: Kami mengirimkan tensor ID token ke model BERT untuk mendapatkan prediksi label token.\n",
    "# Model BERT digunakan untuk tugas Token Classification, di mana setiap token dalam teks diberi label. Kode ini menghitung label token.\n",
    "usia_value = \"\"\n",
    "in_usia_entity = False\n",
    "for token, label in zip(tokens, predicted_labels):\n",
    "    if label == tokenizer.convert_tokens_to_ids('B-USIA'):\n",
    "        in_usia_entity = True\n",
    "        usia_value = token\n",
    "    elif in_usia_entity and label == tokenizer.convert_tokens_to_ids('I-USIA'):\n",
    "        usia_value += token\n",
    "    else:\n",
    "        in_usia_entity = False\n",
    "\n",
    "# Print the extracted USIA value\n",
    "# 8. Get Token Predictions: Kami mengirimkan tensor ID token ke model BERT untuk mendapatkan prediksi label token. \n",
    "# Model BERT digunakan untuk tugas Token Classification, di mana setiap token dalam teks diberi label. Kode ini menghitung label token.\n",
    "# print(str(usia_value) + \" << ini hasil\")\n",
    "print(usia_value)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
